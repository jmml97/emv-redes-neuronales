@book{izenman_modern_2008,
	location = {New York ; London},
	title = {Modern multivariate statistical techniques: regression, classification, and manifold learning},
	isbn = {978-0-387-78188-4},
	series = {Springer texts in statistics},
	shorttitle = {Modern multivariate statistical techniques},
	pagetotal = {731},
	publisher = {Springer},
	author = {Izenman, Alan Julian},
	date = {2008},
	note = {{OCLC}: ocn225427579},
	keywords = {Multivariate analysis, Problems, exercises, etc}
}

@article{mcculloch_logical_1943,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{rosenblatt_perceptron_1958,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@misc{wikipedia_separable,
   author = "Wikipedia",
   title = "Lineare Separierbarkeit --- Wikipedia{,} Die freie Enzyklop√§die",
   year = "2017",
   url = "https://de.wikipedia.org/w/index.php?title=Lineare_Separierbarkeit&oldid=162115781",
   urldate = {2020-01-16}
 }

@artwork{quora_lr_2017,
	title = {In neural networks, why would one use many learning rates in decreasing steps rather than one smooth learning rate decay?},
  author = {Charu Sharma},
	url = {https://www.quora.com/In-neural-networks-why-would-one-use-many-learning-rates-in-decreasing-steps-rather-than-one-smooth-learning-rate-decay},
  date={2017}
}

@artwork{quora_overfitting_2016,
	title = {Which signals do indicate that the convolutional neural network is overfitted?},
  author = {Rafael Cartenet},
	url = {https://www.quora.com/Which-signals-do-indicate-that-the-convolutional-neural-network-is-overfitted},
  date={2016}
}

@artwork{bhande_fit_2018,
	title = {What is underfitting and overfitting in machine learning and how to deal with it.},
  author = {Anup Bhande},
	url = {https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76},
  date={2018}
}

@artwork{rizhevsky_cifar_2013,
	title = {The CIFAR-10 dataset},
  author = {Alex Krizhevsky},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
  date={2013}
}

 @misc{wiki_single_2015,
   author = "Wikimedia Commons",
   title = "File:SingleLayerNeuralNetwork.png --- Wikimedia Commons{,} the free media repository",
   year = "2015",
   url = "https://commons.wikimedia.org/w/index.php?title=File:SingleLayerNeuralNetwork.png&oldid=157022416",
 }

@artwork{machine_image_2017,
	title = {Image Filtering},
  author = {Machine Learning Guru},
	url = {http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html},
  date={2017}
}

@artwork{arden_autoencoder_2017,
	title = {Applied Deep Learning -- Part 3: Autoencoders},
  author = {Arden Dertat},
	url = {https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798},
  date={2017}
}

@article{rizhevsky_learning_2009,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  date={2009},
  url={https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}

@article{lecun_1998_gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@artwork{dhp1080_neurona_2007,
	title = {Neurona},
	url = {https://es.wikipedia.org/wiki/Archivo:Neurona.svg},
	author = {{Dhp1080} and {Acracia}},
	urldate = {2020-01-16},
	date = {2007-01-14}
}

@artwork{nigam_understanding_2018,
	title = {Understanding Neural Networks. From neuron to RNN, CNN, and Deep Learning},
	url = {https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90},
	author = {vibhor nigam},
	urldate = {2020-01-20},
	date = {2018}
}

@incollection{lu_expressive_2017,
	title = {The Expressive Power of Neural Networks: A View from the Width},
	url = {http://papers.nips.cc/paper/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf},
	pages = {6231--6239},
	booktitle = {Advances in Neural Information Processing Systems 30},
	author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
	date = {2017}
}

% https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8
% https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90
